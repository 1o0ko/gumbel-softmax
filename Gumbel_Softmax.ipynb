{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gumbel Softmax",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1o0ko/gumbel-softmax/blob/master/Gumbel_Softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rP6NiDdRZWRw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gumbel Distribution"
      ]
    },
    {
      "metadata": {
        "id": "mKPaCeMCZWRz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uMrajA-AZWR3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "N = 10**6\n",
        "bins = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iiZqAN-mZWR5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "u = np.random.random((N))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UgNzu0c6ZWR8",
        "colab_type": "code",
        "outputId": "babd6e43-b843-44b7-e9f7-0522a2aece41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(u, bins=bins);\n",
        "plt.title(\"Uniform distribution\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Uniform distribution')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEHCAYAAABBW1qbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2a592ec278>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "7VWmLpf5ZWSB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "g = -np.log(-np.log(u))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DccGBwaXZWSE",
        "colab_type": "code",
        "outputId": "bdcf7f9d-58ae-4a7e-f06f-fb4da4f476b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "cell_type": "code",
      "source": [
        "plt.hist(g, bins=bins);\n",
        "plt.title(\"Gumbel distribution\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Gumbel distribution')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEHCAYAAACwUAEWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2a3e478da0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "YECJp3NRZWSI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Discrete Sampling\n",
        "\n",
        "Goal: Sampling from a discrete distribution parametrized by $K$ unnormalized log-probabilities $\\alpha_k$:\n",
        "\n",
        "$$\n",
        "    \\pi_k =\\frac{1}{z} \\exp(\\alpha_k),  \\quad \\textrm{where} \\, z=\\sum_j^K \\exp(x_j)\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "zsYlh1Y-ZWSI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# The Gumbel Distribution tidck\n",
        "\n",
        "Let $X$ be a discrete random variable with $P(X=k)\\propto \\alpha_k$ random variable and let $\\{G_k\\}_{k\\leq K}$\n",
        "be an i.i.d sequence of standard Gumbel random variables. \n",
        "\n",
        "Then:\n",
        "$$\n",
        "    X = \\arg\\max_k(\\log \\alpha_k + G_k)\n",
        "$$\n",
        "\n",
        "The algorithm\n",
        "* draw Gumbel noise by transforming uniform samples\n",
        "* add the noise to the $\\log \\alpha_k$\n",
        "* take value k that produces the maximum"
      ]
    },
    {
      "metadata": {
        "id": "nZGj7NfeZWSK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gumbel(shape, eps=1e-10):\n",
        "    \"\"\" Sample from Gumbel(0, 1)\"\"\"\n",
        "    u =  np.random.random(shape)\n",
        "    g = -np.log(-np.log(u + eps) + eps)\n",
        "    return g\n",
        "    \n",
        "def gumbel_max_sample(x, is_prob=False):\n",
        "    \"\"\" Draw a sample from P(X=k) prop x_k \"\"\"\n",
        "    if is_prob:\n",
        "        x = np.log(x)\n",
        "        \n",
        "    g = gumbel(shape=x.shape)\n",
        "    return (g + x).argmax(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yADc4BQCZWSN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Relaxing the discretness\n",
        "\n",
        "The main idea\n",
        "\n",
        "* Any discrete random variable can be always expressed as one hot-vector\n",
        "* the convex hull of the set of one-hot vector is the probability simplex:\n",
        "$$\n",
        "\\Delta^{K-1} = \\big\\{ x \\in R^K_+, \\, \\sum_{k=1}^K x_k = 1 \\big\\}\n",
        "$$\n",
        "\n",
        "Therefore, a natural way to extend (or ‘relax’) a discrete random variables is by allowing it to take values in the *probability simplex*. We can can consider a soft-max map, to extend the values of a discrete random variable\n",
        "\n",
        "$$\n",
        "    f_\\tau (x)_k = \\frac{\\exp(x_k / \\tau)}{\\sum_{k=1}^K\\exp(x_k / \\tau)}\n",
        "$$"
      ]
    },
    {
      "metadata": {
        "id": "KHrjePUjZWSN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def softmax(X, temperature = 1.0, axis = None):\n",
        "    \"\"\"\n",
        "    Compute the softmax of each element along an axis of X.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: ND-Array. Probably should be floats. \n",
        "    temperature (optional): float parameter, used as a multiplier\n",
        "        prior to exponentiation. Default = 1.0\n",
        "    axis (optional): axis to compute values along. Default is the \n",
        "        first non-singleton axis.\n",
        "\n",
        "    Returns an array the same size as X. The result will sum to 1\n",
        "    along the specified axis.\n",
        "    \"\"\"\n",
        "    y = np.atleast_2d(X)\n",
        "\n",
        "    if axis is None:\n",
        "        axis = next(j[0] for j in enumerate(y.shape) if j[1] > 1)\n",
        "\n",
        "    y = y / float(temperature)\n",
        "    y = y - np.expand_dims(np.max(y, axis = axis), axis)\n",
        "    y = np.exp(y)\n",
        "\n",
        "    # take the sum along the specified axis\n",
        "    p = y / np.expand_dims(np.sum(y, axis = axis), axis)\n",
        "\n",
        "    if len(X.shape) == 1: \n",
        "        p = p.flatten()\n",
        "\n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Ha-AxKJZWSR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gumbel_softmax_sample(logits, temperature=1): \n",
        "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
        "    y = logits + gumbel(np.shape(logits))\n",
        "    return softmax(y, temperature=temperature)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4GncsEIZWSU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# An example sampling"
      ]
    },
    {
      "metadata": {
        "id": "P27tAytVZWSV",
        "colab_type": "code",
        "outputId": "b2f3e821-82f2-491e-8971-7d656940d0dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "cell_type": "code",
      "source": [
        "logits = [1, 4, 5, 1, 2]\n",
        "classes = range(1, len(logits) + 1)\n",
        "sample = gumbel_softmax_sample(logits, temperature=1)\n",
        "\n",
        "plt.title(\"Sample from Gumbel-Softmax distribution\")\n",
        "plt.bar(classes, sample);"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEHCAYAAACzy817AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2a3bb9a080>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "LtNsXGRhZysb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gumbel-Softmax in PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "tQPOo53IaTMe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "8a02ea25-b11c-4e51-e2ec-a9e7a474da41"
      },
      "cell_type": "code",
      "source": [
        "! pip3 install torch torchvision\n",
        "! pip3 install pillow"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (5.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.11.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (5.3.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "miRy5qscZ11X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "def sample_gumbel(shape, eps=1e-20, device='cuda'):\n",
        "    ''' Sample from Gumbel(0, 1) ''' \n",
        "    U = torch.rand(shape).to(device)    \n",
        "    return -torch.log(-torch.log(U + eps) + eps)\n",
        "\n",
        "def gumbel_softmax_sample(logits, temperature):\n",
        "    ''' Draw a sample from the Gumbel-Softmax distribution ''' \n",
        "    y = logits + sample_gumbel(logits.size())\n",
        "    return F.softmax(y / temperature, dim=-1)\n",
        "\n",
        "def gumbel_softmax(logits, temperature, hard=False):\n",
        "    '''Sample from the Gumbel-Softmax distribution and optionally discretize.\n",
        "    \n",
        "    Args:\n",
        "      logits: [batch_size, n_class] unnormalized log-probs\n",
        "      temperature: non-negative scalar\n",
        "      hard: if True, take argmax, but differentiate w.r.t. soft sample y\n",
        "    \n",
        "    Returns:\n",
        "      [batch_size, n_class] sample from the Gumbel-Softmax distribution.\n",
        "      If hard=True, then the returned sample will be one-hot, otherwise it will\n",
        "      be a probabilitiy distribution that sums to 1 across classes\n",
        "    '''\n",
        "    y = gumbel_softmax_sample(logits, temperature)\n",
        "    if hard:\n",
        "      shape = y.size()\n",
        "      _, ind = y.max(dim=-1)\n",
        "      y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
        "      y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
        "      y_hard = y_hard.view(*shape)\n",
        "      y =  (y_hard - y).detach() + y      \n",
        "    \n",
        "    return y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QbGgLoVXbjiv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = torch.Tensor([1, 1.3]).cuda()\n",
        "classes = range(1, len(logits) + 1)\n",
        "sample = gumbel_softmax(logits, temperature=1, hard=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aN169tF1dyb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "d01c86f6-1207-4166-e7de-7f238358241e"
      },
      "cell_type": "code",
      "source": [
        "plt.title(\"Sample from Gumbel-Softmax distribution\")\n",
        "plt.bar(classes, sample);"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAEHCAYAAACzy817AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f2a071cc780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "vYa75gQp8YyC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data"
      ]
    },
    {
      "metadata": {
        "id": "fqRm-9X58nPK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory = '/content/results/'\n",
        "if not os.path.exists(directory):\n",
        "  os.makedirs(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zr6Tmz8IKWm-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.utils.data\n",
        "\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YoD8Eq4g8Zqx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
        "    batch_size=128, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=128, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TGpRJ2Xi8Z_u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Binary-VAE"
      ]
    },
    {
      "metadata": {
        "id": "I8vrJpJL9dc_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ANNEAL_RATE = 0.00003\n",
        "\n",
        "\n",
        "class BinaryVAE(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(BinaryVAE, self).__init__()\n",
        "        \n",
        "        # meta info\n",
        "        self.latent_dim = latent_dim\n",
        "        \n",
        "        self.encoder = nn.Sequential(\n",
        "          nn.Linear(784, 512),\n",
        "          nn.ReLU(True),\n",
        "          nn.Linear(512, 256),\n",
        "          nn.ReLU(True),\n",
        "          nn.Linear(256, latent_dim * 2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim * 2, 256),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(512, 784),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "      \n",
        "\n",
        "    def get_codes(self, x, temp):\n",
        "        logits = self.encoder(x.view(-1, 784))\n",
        "        logits = logits.view(-1, self.latent_dim, 2)\n",
        "        \n",
        "        codes = gumbel_softmax(logits, temp, hard=True)\n",
        "        codes = codes[:,:, 0] # take the first column of the binary variable\n",
        "        \n",
        "        return codes\n",
        "      \n",
        "    def forward(self, x, temp, hard):\n",
        " \n",
        "        q = self.encoder(x.view(-1, 784))\n",
        "        q_y = q.view(-1, self.latent_dim, 2)\n",
        "        \n",
        "\n",
        "        z = gumbel_softmax(q_y, temp, hard)\n",
        "        z = z.view(-1, self.latent_dim * 2)\n",
        "        \n",
        "        x_recon = self.decoder(z)\n",
        "        \n",
        "        q_y = F.softmax(q_y, dim=-1).reshape(*q.size())\n",
        "\n",
        "        return x_recon, q_y\n",
        "\n",
        "\n",
        "\n",
        "def loss_function(recon_x, x, q_y):\n",
        "    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False) / x.shape[0]\n",
        "\n",
        "    log_ratio = torch.log(q_y * 2 + 1e-20)\n",
        "    KLD = torch.sum(q_y * log_ratio, dim=-1).mean()\n",
        "\n",
        "    return BCE + KLD\n",
        "\n",
        "\n",
        "  \n",
        "def train(epoch, model, train_loader, optimizer, temp, cuda=True, hard=False):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        if cuda:\n",
        "            data = data.cuda()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        recon_batch, q_y = model(data, temp, hard)\n",
        "        \n",
        "        loss = loss_function(recon_batch, data, q_y)\n",
        "        loss.backward()\n",
        "        \n",
        "        train_loss += loss.item() * len(data)\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 100 == 1:\n",
        "            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * batch_idx), temp_min)\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                       100. * batch_idx / len(train_loader),\n",
        "                       loss.item()))\n",
        "\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
        "        epoch, train_loss / len(train_loader.dataset)))\n",
        "\n",
        "\n",
        "\n",
        "def test(epoch, model, test_loader, temp, cuda=True, hard=False):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "  \n",
        "    for i, (data, _) in enumerate(test_loader):\n",
        "        if cuda:\n",
        "          data = data.cuda()\n",
        "  \n",
        "        recon_batch, qy = model(data, temp, hard)\n",
        "        test_loss += loss_function(recon_batch, data, qy).item() * len(data)\n",
        "        \n",
        "        if i % 100 == 1:\n",
        "            temp = np.maximum(temp * np.exp(-ANNEAL_RATE * i), temp_min)\n",
        "        \n",
        "        if i == 0:\n",
        "            n = min(data.size(0), 8)\n",
        "            comparison = torch.cat([data[:n],\n",
        "                                    recon_batch.view(128, 1, 28, 28)[:n]])\n",
        "            save_image(comparison.data.cpu(),\n",
        "                       f\"/content/results/reconstruction_{epoch:03d}.png\", nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8IeQZxcj9hvw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "9f1a9822-97a2-45af-a41a-328d779868e9"
      },
      "cell_type": "code",
      "source": [
        "epochs = 50\n",
        "prec = math.ceil(math.log10(epochs / 100))\n",
        "\n",
        "\n",
        "latent_dim = 32\n",
        "temp = 1.0\n",
        "temp_min = 0.5\n",
        "\n",
        "\n",
        "model = BinaryVAE(latent_dim)\n",
        "model.to(device)"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BinaryVAE(\n",
              "  (encoder): Sequential(\n",
              "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): ReLU(inplace)\n",
              "    (4): Linear(in_features=256, out_features=64, bias=True)\n",
              "  )\n",
              "  (decoder): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=256, bias=True)\n",
              "    (1): ReLU(inplace)\n",
              "    (2): Linear(in_features=256, out_features=512, bias=True)\n",
              "    (3): ReLU(inplace)\n",
              "    (4): Linear(in_features=512, out_features=784, bias=True)\n",
              "    (5): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "metadata": {
        "id": "uucreost9qkB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6021
        },
        "outputId": "69a8564f-65e0-4298-adaa-1e8bef9e081e"
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(epoch, model, train_loader, optimizer, temp, True)\n",
        "    test(epoch, model, test_loader, temp, True)\n",
        "\n",
        "    M = 64 * latent_dim\n",
        "    np_y = np.zeros((M, 2), dtype=np.float32)\n",
        "    np_y[range(M), np.random.choice(2, M)] = 1\n",
        "    np_y = np.reshape(np_y, [M // latent_dim, latent_dim, 2])\n",
        "\n",
        "    sample = torch.from_numpy(np_y).view(M // latent_dim, latent_dim * 2)\n",
        "    sample = sample.to(device)\n",
        "    sample = model.decoder(sample).cpu()\n",
        "    \n",
        "    save_image(\n",
        "        sample.data.view(M // latent_dim, 1, 28, 28),\n",
        "        f\"/content/results/sample_{epoch:03d}.png\")"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 542.468079\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 208.040283\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 186.274628\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 184.921844\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 171.275177\n",
            "====> Epoch: 1 Average loss: 195.6977\n",
            "====> Test set loss: 166.4500\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 159.574493\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 163.180267\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 149.806229\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 148.963547\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 141.256119\n",
            "====> Epoch: 2 Average loss: 150.8755\n",
            "====> Test set loss: 140.3907\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 138.322128\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 138.468735\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 137.284088\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 140.577408\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 132.205795\n",
            "====> Epoch: 3 Average loss: 136.1242\n",
            "====> Test set loss: 130.6723\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 132.398727\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 136.526489\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 125.085220\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 122.312843\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 122.278862\n",
            "====> Epoch: 4 Average loss: 128.0379\n",
            "====> Test set loss: 124.1862\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 124.871269\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 126.411171\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 118.996750\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 118.287003\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 125.821579\n",
            "====> Epoch: 5 Average loss: 123.0555\n",
            "====> Test set loss: 119.9232\n",
            "Train Epoch: 6 [0/60000 (0%)]\tLoss: 117.264946\n",
            "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 121.852135\n",
            "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 123.996803\n",
            "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 119.364937\n",
            "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 121.432053\n",
            "====> Epoch: 6 Average loss: 119.0253\n",
            "====> Test set loss: 116.6455\n",
            "Train Epoch: 7 [0/60000 (0%)]\tLoss: 119.339882\n",
            "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 119.244461\n",
            "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 119.341324\n",
            "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 116.171059\n",
            "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 112.013351\n",
            "====> Epoch: 7 Average loss: 116.5011\n",
            "====> Test set loss: 114.8562\n",
            "Train Epoch: 8 [0/60000 (0%)]\tLoss: 112.409630\n",
            "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 113.434174\n",
            "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 115.644653\n",
            "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 114.121803\n",
            "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 118.737106\n",
            "====> Epoch: 8 Average loss: 114.6532\n",
            "====> Test set loss: 113.3677\n",
            "Train Epoch: 9 [0/60000 (0%)]\tLoss: 110.245140\n",
            "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 114.773590\n",
            "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 110.692337\n",
            "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 109.878677\n",
            "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 106.563393\n",
            "====> Epoch: 9 Average loss: 113.2545\n",
            "====> Test set loss: 112.4167\n",
            "Train Epoch: 10 [0/60000 (0%)]\tLoss: 109.567230\n",
            "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 116.099014\n",
            "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 111.424225\n",
            "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 109.004990\n",
            "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 109.865105\n",
            "====> Epoch: 10 Average loss: 112.0825\n",
            "====> Test set loss: 111.0965\n",
            "Train Epoch: 11 [0/60000 (0%)]\tLoss: 109.696877\n",
            "Train Epoch: 11 [12800/60000 (21%)]\tLoss: 108.370522\n",
            "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 111.137688\n",
            "Train Epoch: 11 [38400/60000 (64%)]\tLoss: 115.078209\n",
            "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 110.032455\n",
            "====> Epoch: 11 Average loss: 111.1082\n",
            "====> Test set loss: 110.1241\n",
            "Train Epoch: 12 [0/60000 (0%)]\tLoss: 111.595497\n",
            "Train Epoch: 12 [12800/60000 (21%)]\tLoss: 115.146362\n",
            "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 109.085762\n",
            "Train Epoch: 12 [38400/60000 (64%)]\tLoss: 118.925552\n",
            "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 108.336578\n",
            "====> Epoch: 12 Average loss: 110.1958\n",
            "====> Test set loss: 109.9965\n",
            "Train Epoch: 13 [0/60000 (0%)]\tLoss: 107.978325\n",
            "Train Epoch: 13 [12800/60000 (21%)]\tLoss: 106.856461\n",
            "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 112.491211\n",
            "Train Epoch: 13 [38400/60000 (64%)]\tLoss: 110.352211\n",
            "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 107.473419\n",
            "====> Epoch: 13 Average loss: 109.4710\n",
            "====> Test set loss: 108.9833\n",
            "Train Epoch: 14 [0/60000 (0%)]\tLoss: 109.140755\n",
            "Train Epoch: 14 [12800/60000 (21%)]\tLoss: 106.925179\n",
            "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 109.490776\n",
            "Train Epoch: 14 [38400/60000 (64%)]\tLoss: 107.468132\n",
            "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 107.688004\n",
            "====> Epoch: 14 Average loss: 108.8048\n",
            "====> Test set loss: 108.4675\n",
            "Train Epoch: 15 [0/60000 (0%)]\tLoss: 107.814049\n",
            "Train Epoch: 15 [12800/60000 (21%)]\tLoss: 108.393402\n",
            "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 102.523361\n",
            "Train Epoch: 15 [38400/60000 (64%)]\tLoss: 111.657074\n",
            "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 103.307663\n",
            "====> Epoch: 15 Average loss: 108.2319\n",
            "====> Test set loss: 107.8503\n",
            "Train Epoch: 16 [0/60000 (0%)]\tLoss: 106.650101\n",
            "Train Epoch: 16 [12800/60000 (21%)]\tLoss: 112.827919\n",
            "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 111.178589\n",
            "Train Epoch: 16 [38400/60000 (64%)]\tLoss: 108.239861\n",
            "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 108.029266\n",
            "====> Epoch: 16 Average loss: 107.7393\n",
            "====> Test set loss: 107.3608\n",
            "Train Epoch: 17 [0/60000 (0%)]\tLoss: 106.892357\n",
            "Train Epoch: 17 [12800/60000 (21%)]\tLoss: 108.476440\n",
            "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 106.224991\n",
            "Train Epoch: 17 [38400/60000 (64%)]\tLoss: 102.090485\n",
            "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 103.229507\n",
            "====> Epoch: 17 Average loss: 107.1574\n",
            "====> Test set loss: 107.0433\n",
            "Train Epoch: 18 [0/60000 (0%)]\tLoss: 108.036362\n",
            "Train Epoch: 18 [12800/60000 (21%)]\tLoss: 105.666061\n",
            "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 102.399132\n",
            "Train Epoch: 18 [38400/60000 (64%)]\tLoss: 104.994392\n",
            "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 103.133919\n",
            "====> Epoch: 18 Average loss: 106.7556\n",
            "====> Test set loss: 107.0790\n",
            "Train Epoch: 19 [0/60000 (0%)]\tLoss: 101.159691\n",
            "Train Epoch: 19 [12800/60000 (21%)]\tLoss: 109.055893\n",
            "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 105.969742\n",
            "Train Epoch: 19 [38400/60000 (64%)]\tLoss: 107.965950\n",
            "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 107.159637\n",
            "====> Epoch: 19 Average loss: 106.3893\n",
            "====> Test set loss: 106.3674\n",
            "Train Epoch: 20 [0/60000 (0%)]\tLoss: 103.968124\n",
            "Train Epoch: 20 [12800/60000 (21%)]\tLoss: 104.334839\n",
            "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 110.103806\n",
            "Train Epoch: 20 [38400/60000 (64%)]\tLoss: 101.469254\n",
            "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 108.799049\n",
            "====> Epoch: 20 Average loss: 105.9659\n",
            "====> Test set loss: 105.8914\n",
            "Train Epoch: 21 [0/60000 (0%)]\tLoss: 108.184006\n",
            "Train Epoch: 21 [12800/60000 (21%)]\tLoss: 103.204582\n",
            "Train Epoch: 21 [25600/60000 (43%)]\tLoss: 107.976601\n",
            "Train Epoch: 21 [38400/60000 (64%)]\tLoss: 104.073456\n",
            "Train Epoch: 21 [51200/60000 (85%)]\tLoss: 104.002472\n",
            "====> Epoch: 21 Average loss: 105.6033\n",
            "====> Test set loss: 105.7517\n",
            "Train Epoch: 22 [0/60000 (0%)]\tLoss: 108.674149\n",
            "Train Epoch: 22 [12800/60000 (21%)]\tLoss: 107.945045\n",
            "Train Epoch: 22 [25600/60000 (43%)]\tLoss: 106.541306\n",
            "Train Epoch: 22 [38400/60000 (64%)]\tLoss: 104.166046\n",
            "Train Epoch: 22 [51200/60000 (85%)]\tLoss: 107.555115\n",
            "====> Epoch: 22 Average loss: 105.2739\n",
            "====> Test set loss: 105.4508\n",
            "Train Epoch: 23 [0/60000 (0%)]\tLoss: 102.465721\n",
            "Train Epoch: 23 [12800/60000 (21%)]\tLoss: 104.810211\n",
            "Train Epoch: 23 [25600/60000 (43%)]\tLoss: 102.420792\n",
            "Train Epoch: 23 [38400/60000 (64%)]\tLoss: 111.917435\n",
            "Train Epoch: 23 [51200/60000 (85%)]\tLoss: 107.873970\n",
            "====> Epoch: 23 Average loss: 104.9627\n",
            "====> Test set loss: 105.2417\n",
            "Train Epoch: 24 [0/60000 (0%)]\tLoss: 103.350380\n",
            "Train Epoch: 24 [12800/60000 (21%)]\tLoss: 104.030289\n",
            "Train Epoch: 24 [25600/60000 (43%)]\tLoss: 105.578552\n",
            "Train Epoch: 24 [38400/60000 (64%)]\tLoss: 101.171738\n",
            "Train Epoch: 24 [51200/60000 (85%)]\tLoss: 100.457336\n",
            "====> Epoch: 24 Average loss: 104.7126\n",
            "====> Test set loss: 105.1186\n",
            "Train Epoch: 25 [0/60000 (0%)]\tLoss: 101.108322\n",
            "Train Epoch: 25 [12800/60000 (21%)]\tLoss: 104.645294\n",
            "Train Epoch: 25 [25600/60000 (43%)]\tLoss: 111.155251\n",
            "Train Epoch: 25 [38400/60000 (64%)]\tLoss: 102.256973\n",
            "Train Epoch: 25 [51200/60000 (85%)]\tLoss: 102.673447\n",
            "====> Epoch: 25 Average loss: 104.4468\n",
            "====> Test set loss: 104.7079\n",
            "Train Epoch: 26 [0/60000 (0%)]\tLoss: 102.767075\n",
            "Train Epoch: 26 [12800/60000 (21%)]\tLoss: 103.695915\n",
            "Train Epoch: 26 [25600/60000 (43%)]\tLoss: 105.867821\n",
            "Train Epoch: 26 [38400/60000 (64%)]\tLoss: 103.561325\n",
            "Train Epoch: 26 [51200/60000 (85%)]\tLoss: 105.275192\n",
            "====> Epoch: 26 Average loss: 104.1956\n",
            "====> Test set loss: 104.5726\n",
            "Train Epoch: 27 [0/60000 (0%)]\tLoss: 100.827759\n",
            "Train Epoch: 27 [12800/60000 (21%)]\tLoss: 105.378677\n",
            "Train Epoch: 27 [25600/60000 (43%)]\tLoss: 105.735451\n",
            "Train Epoch: 27 [38400/60000 (64%)]\tLoss: 102.239990\n",
            "Train Epoch: 27 [51200/60000 (85%)]\tLoss: 109.071754\n",
            "====> Epoch: 27 Average loss: 103.9290\n",
            "====> Test set loss: 104.3902\n",
            "Train Epoch: 28 [0/60000 (0%)]\tLoss: 100.555328\n",
            "Train Epoch: 28 [12800/60000 (21%)]\tLoss: 101.975586\n",
            "Train Epoch: 28 [25600/60000 (43%)]\tLoss: 99.261314\n",
            "Train Epoch: 28 [38400/60000 (64%)]\tLoss: 104.031914\n",
            "Train Epoch: 28 [51200/60000 (85%)]\tLoss: 106.459541\n",
            "====> Epoch: 28 Average loss: 103.6314\n",
            "====> Test set loss: 104.0725\n",
            "Train Epoch: 29 [0/60000 (0%)]\tLoss: 97.770447\n",
            "Train Epoch: 29 [12800/60000 (21%)]\tLoss: 106.134392\n",
            "Train Epoch: 29 [25600/60000 (43%)]\tLoss: 101.720726\n",
            "Train Epoch: 29 [38400/60000 (64%)]\tLoss: 105.797081\n",
            "Train Epoch: 29 [51200/60000 (85%)]\tLoss: 105.994827\n",
            "====> Epoch: 29 Average loss: 103.4818\n",
            "====> Test set loss: 103.8395\n",
            "Train Epoch: 30 [0/60000 (0%)]\tLoss: 102.383362\n",
            "Train Epoch: 30 [12800/60000 (21%)]\tLoss: 104.402031\n",
            "Train Epoch: 30 [25600/60000 (43%)]\tLoss: 101.661591\n",
            "Train Epoch: 30 [38400/60000 (64%)]\tLoss: 106.390656\n",
            "Train Epoch: 30 [51200/60000 (85%)]\tLoss: 106.129829\n",
            "====> Epoch: 30 Average loss: 103.2417\n",
            "====> Test set loss: 103.5855\n",
            "Train Epoch: 31 [0/60000 (0%)]\tLoss: 103.381966\n",
            "Train Epoch: 31 [12800/60000 (21%)]\tLoss: 102.151299\n",
            "Train Epoch: 31 [25600/60000 (43%)]\tLoss: 102.928810\n",
            "Train Epoch: 31 [38400/60000 (64%)]\tLoss: 98.788177\n",
            "Train Epoch: 31 [51200/60000 (85%)]\tLoss: 107.060478\n",
            "====> Epoch: 31 Average loss: 102.9187\n",
            "====> Test set loss: 103.5809\n",
            "Train Epoch: 32 [0/60000 (0%)]\tLoss: 107.523712\n",
            "Train Epoch: 32 [12800/60000 (21%)]\tLoss: 104.284630\n",
            "Train Epoch: 32 [25600/60000 (43%)]\tLoss: 102.459755\n",
            "Train Epoch: 32 [38400/60000 (64%)]\tLoss: 103.068916\n",
            "Train Epoch: 32 [51200/60000 (85%)]\tLoss: 109.029518\n",
            "====> Epoch: 32 Average loss: 102.7866\n",
            "====> Test set loss: 103.2903\n",
            "Train Epoch: 33 [0/60000 (0%)]\tLoss: 100.603287\n",
            "Train Epoch: 33 [12800/60000 (21%)]\tLoss: 102.121674\n",
            "Train Epoch: 33 [25600/60000 (43%)]\tLoss: 97.294662\n",
            "Train Epoch: 33 [38400/60000 (64%)]\tLoss: 106.238327\n",
            "Train Epoch: 33 [51200/60000 (85%)]\tLoss: 105.177780\n",
            "====> Epoch: 33 Average loss: 102.5761\n",
            "====> Test set loss: 103.5706\n",
            "Train Epoch: 34 [0/60000 (0%)]\tLoss: 102.688614\n",
            "Train Epoch: 34 [12800/60000 (21%)]\tLoss: 104.483360\n",
            "Train Epoch: 34 [25600/60000 (43%)]\tLoss: 103.982307\n",
            "Train Epoch: 34 [38400/60000 (64%)]\tLoss: 97.747932\n",
            "Train Epoch: 34 [51200/60000 (85%)]\tLoss: 99.783806\n",
            "====> Epoch: 34 Average loss: 102.3374\n",
            "====> Test set loss: 102.9539\n",
            "Train Epoch: 35 [0/60000 (0%)]\tLoss: 101.696617\n",
            "Train Epoch: 35 [12800/60000 (21%)]\tLoss: 99.168938\n",
            "Train Epoch: 35 [25600/60000 (43%)]\tLoss: 102.471428\n",
            "Train Epoch: 35 [38400/60000 (64%)]\tLoss: 103.645149\n",
            "Train Epoch: 35 [51200/60000 (85%)]\tLoss: 98.638092\n",
            "====> Epoch: 35 Average loss: 102.1495\n",
            "====> Test set loss: 103.0290\n",
            "Train Epoch: 36 [0/60000 (0%)]\tLoss: 101.144043\n",
            "Train Epoch: 36 [12800/60000 (21%)]\tLoss: 100.836349\n",
            "Train Epoch: 36 [25600/60000 (43%)]\tLoss: 102.223503\n",
            "Train Epoch: 36 [38400/60000 (64%)]\tLoss: 102.049774\n",
            "Train Epoch: 36 [51200/60000 (85%)]\tLoss: 100.029778\n",
            "====> Epoch: 36 Average loss: 101.9534\n",
            "====> Test set loss: 102.5960\n",
            "Train Epoch: 37 [0/60000 (0%)]\tLoss: 100.656631\n",
            "Train Epoch: 37 [12800/60000 (21%)]\tLoss: 102.139473\n",
            "Train Epoch: 37 [25600/60000 (43%)]\tLoss: 96.175697\n",
            "Train Epoch: 37 [38400/60000 (64%)]\tLoss: 102.154472\n",
            "Train Epoch: 37 [51200/60000 (85%)]\tLoss: 101.109207\n",
            "====> Epoch: 37 Average loss: 101.7833\n",
            "====> Test set loss: 102.6500\n",
            "Train Epoch: 38 [0/60000 (0%)]\tLoss: 105.724220\n",
            "Train Epoch: 38 [12800/60000 (21%)]\tLoss: 98.358833\n",
            "Train Epoch: 38 [25600/60000 (43%)]\tLoss: 102.579681\n",
            "Train Epoch: 38 [38400/60000 (64%)]\tLoss: 95.237396\n",
            "Train Epoch: 38 [51200/60000 (85%)]\tLoss: 101.702286\n",
            "====> Epoch: 38 Average loss: 101.6674\n",
            "====> Test set loss: 102.4365\n",
            "Train Epoch: 39 [0/60000 (0%)]\tLoss: 101.964180\n",
            "Train Epoch: 39 [12800/60000 (21%)]\tLoss: 99.510399\n",
            "Train Epoch: 39 [25600/60000 (43%)]\tLoss: 101.633850\n",
            "Train Epoch: 39 [38400/60000 (64%)]\tLoss: 101.161194\n",
            "Train Epoch: 39 [51200/60000 (85%)]\tLoss: 100.296959\n",
            "====> Epoch: 39 Average loss: 101.4476\n",
            "====> Test set loss: 102.1722\n",
            "Train Epoch: 40 [0/60000 (0%)]\tLoss: 93.093468\n",
            "Train Epoch: 40 [12800/60000 (21%)]\tLoss: 98.081512\n",
            "Train Epoch: 40 [25600/60000 (43%)]\tLoss: 106.314041\n",
            "Train Epoch: 40 [38400/60000 (64%)]\tLoss: 103.190781\n",
            "Train Epoch: 40 [51200/60000 (85%)]\tLoss: 103.749725\n",
            "====> Epoch: 40 Average loss: 101.2508\n",
            "====> Test set loss: 102.1182\n",
            "Train Epoch: 41 [0/60000 (0%)]\tLoss: 101.331276\n",
            "Train Epoch: 41 [12800/60000 (21%)]\tLoss: 99.668129\n",
            "Train Epoch: 41 [25600/60000 (43%)]\tLoss: 103.347740\n",
            "Train Epoch: 41 [38400/60000 (64%)]\tLoss: 100.056160\n",
            "Train Epoch: 41 [51200/60000 (85%)]\tLoss: 102.077644\n",
            "====> Epoch: 41 Average loss: 101.1145\n",
            "====> Test set loss: 102.1105\n",
            "Train Epoch: 42 [0/60000 (0%)]\tLoss: 102.384567\n",
            "Train Epoch: 42 [12800/60000 (21%)]\tLoss: 97.395683\n",
            "Train Epoch: 42 [25600/60000 (43%)]\tLoss: 103.328888\n",
            "Train Epoch: 42 [38400/60000 (64%)]\tLoss: 104.019730\n",
            "Train Epoch: 42 [51200/60000 (85%)]\tLoss: 101.449638\n",
            "====> Epoch: 42 Average loss: 100.8880\n",
            "====> Test set loss: 101.5897\n",
            "Train Epoch: 43 [0/60000 (0%)]\tLoss: 99.814812\n",
            "Train Epoch: 43 [12800/60000 (21%)]\tLoss: 102.192726\n",
            "Train Epoch: 43 [25600/60000 (43%)]\tLoss: 101.781342\n",
            "Train Epoch: 43 [38400/60000 (64%)]\tLoss: 99.521202\n",
            "Train Epoch: 43 [51200/60000 (85%)]\tLoss: 106.016403\n",
            "====> Epoch: 43 Average loss: 100.7606\n",
            "====> Test set loss: 101.6487\n",
            "Train Epoch: 44 [0/60000 (0%)]\tLoss: 103.102524\n",
            "Train Epoch: 44 [12800/60000 (21%)]\tLoss: 96.431412\n",
            "Train Epoch: 44 [25600/60000 (43%)]\tLoss: 96.549316\n",
            "Train Epoch: 44 [38400/60000 (64%)]\tLoss: 103.660881\n",
            "Train Epoch: 44 [51200/60000 (85%)]\tLoss: 102.908089\n",
            "====> Epoch: 44 Average loss: 100.5827\n",
            "====> Test set loss: 101.7938\n",
            "Train Epoch: 45 [0/60000 (0%)]\tLoss: 99.422874\n",
            "Train Epoch: 45 [12800/60000 (21%)]\tLoss: 99.873039\n",
            "Train Epoch: 45 [25600/60000 (43%)]\tLoss: 98.942039\n",
            "Train Epoch: 45 [38400/60000 (64%)]\tLoss: 99.867172\n",
            "Train Epoch: 45 [51200/60000 (85%)]\tLoss: 99.831955\n",
            "====> Epoch: 45 Average loss: 100.3786\n",
            "====> Test set loss: 101.7346\n",
            "Train Epoch: 46 [0/60000 (0%)]\tLoss: 100.896446\n",
            "Train Epoch: 46 [12800/60000 (21%)]\tLoss: 102.475220\n",
            "Train Epoch: 46 [25600/60000 (43%)]\tLoss: 98.623627\n",
            "Train Epoch: 46 [38400/60000 (64%)]\tLoss: 98.252243\n",
            "Train Epoch: 46 [51200/60000 (85%)]\tLoss: 104.002708\n",
            "====> Epoch: 46 Average loss: 100.2218\n",
            "====> Test set loss: 101.2286\n",
            "Train Epoch: 47 [0/60000 (0%)]\tLoss: 99.000244\n",
            "Train Epoch: 47 [12800/60000 (21%)]\tLoss: 100.428238\n",
            "Train Epoch: 47 [25600/60000 (43%)]\tLoss: 101.408180\n",
            "Train Epoch: 47 [38400/60000 (64%)]\tLoss: 101.152061\n",
            "Train Epoch: 47 [51200/60000 (85%)]\tLoss: 98.974968\n",
            "====> Epoch: 47 Average loss: 100.1171\n",
            "====> Test set loss: 101.2275\n",
            "Train Epoch: 48 [0/60000 (0%)]\tLoss: 100.551529\n",
            "Train Epoch: 48 [12800/60000 (21%)]\tLoss: 97.700302\n",
            "Train Epoch: 48 [25600/60000 (43%)]\tLoss: 105.148422\n",
            "Train Epoch: 48 [38400/60000 (64%)]\tLoss: 100.053947\n",
            "Train Epoch: 48 [51200/60000 (85%)]\tLoss: 99.010803\n",
            "====> Epoch: 48 Average loss: 99.9551\n",
            "====> Test set loss: 101.0836\n",
            "Train Epoch: 49 [0/60000 (0%)]\tLoss: 103.578224\n",
            "Train Epoch: 49 [12800/60000 (21%)]\tLoss: 100.322952\n",
            "Train Epoch: 49 [25600/60000 (43%)]\tLoss: 96.332481\n",
            "Train Epoch: 49 [38400/60000 (64%)]\tLoss: 100.435890\n",
            "Train Epoch: 49 [51200/60000 (85%)]\tLoss: 101.403511\n",
            "====> Epoch: 49 Average loss: 99.8238\n",
            "====> Test set loss: 100.7249\n",
            "Train Epoch: 50 [0/60000 (0%)]\tLoss: 103.125900\n",
            "Train Epoch: 50 [12800/60000 (21%)]\tLoss: 98.542488\n",
            "Train Epoch: 50 [25600/60000 (43%)]\tLoss: 99.773834\n",
            "Train Epoch: 50 [38400/60000 (64%)]\tLoss: 99.329750\n",
            "Train Epoch: 50 [51200/60000 (85%)]\tLoss: 100.310661\n",
            "====> Epoch: 50 Average loss: 99.6858\n",
            "====> Test set loss: 100.8460\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-3kLXZDKewuq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_loader))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YHsXQhB3n7fN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = batch[0][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gjiw0BkytOHV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09658484-b649-4157-ea1f-26ccd1e65d64"
      },
      "cell_type": "code",
      "source": [
        "# check if the codes are the same\n",
        "a = model.get_codes(x.cuda(), 0.1)\n",
        "b = model.get_codes(x.cuda(), 0.1)\n",
        "\n",
        "torch.sum(a != b).data.cpu()"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    }
  ]
}